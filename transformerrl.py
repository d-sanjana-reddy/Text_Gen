# -*- coding: utf-8 -*-
"""TransformerRL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yETX1yQ5ZjjswpPZ6g05oFyy0axtYWYg
"""

# CELL 1: Load and Preprocess Data
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import re

# Load dataset
df = pd.read_csv('/content/TXT_GEN.csv')

# Clean text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

df['text'] = df['text'].apply(clean_text)
df.dropna(subset=['text', 'subject'], inplace=True)

# Encode labels
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['subject'])

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)

print("Data preprocessing complete.")

# CELL 2: Text Generation using Transformer (GPT-2)
!pip install transformers --quiet
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load GPT-2
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')
model.eval()

# Generate sample text from training data prefix
def generate_text(prompt_text, max_length=100):
    inputs = tokenizer.encode(prompt_text, return_tensors='pt')
    outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_k=50)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

sample_prompt = X_train.iloc[0][:100]
print("\nGenerated text:\n", generate_text(sample_prompt))

# CELL 3: Q-Learning Placeholder
print("Q-learning logic placeholder. In actual RL setup, this could optimize prompt selection or model parameters.")

# CELL 4: Feature Extraction using BERT Embeddings
!pip install sentence-transformers --quiet
from sentence_transformers import SentenceTransformer

# Load BERT-based model
bert_model = SentenceTransformer('all-MiniLM-L6-v2')

# Generate embeddings
X_train_embed = bert_model.encode(X_train.tolist(), show_progress_bar=True)
X_test_embed = bert_model.encode(X_test.tolist(), show_progress_bar=True)

print("Text embeddings generated.")

# CELL 5: Logistic Regression Classification
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_embed, y_train)
y_pred_lr = lr_model.predict(X_test_embed)

print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_lr))
print(classification_report(y_test, y_pred_lr))

# CELL 6: Random Forest Classification
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier()
rf_model.fit(X_train_embed, y_train)
y_pred_rf = rf_model.predict(X_test_embed)

print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

# CELL 7: Support Vector Machine Classification
from sklearn.svm import SVC

svm_model = SVC()
svm_model.fit(X_train_embed, y_train)
y_pred_svm = svm_model.predict(X_test_embed)

print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))

# CELL 8: Naive Bayes Classification
from sklearn.naive_bayes import GaussianNB

nb_model = GaussianNB()
nb_model.fit(X_train_embed, y_train)
y_pred_nb = nb_model.predict(X_test_embed)

print("Naive Bayes Accuracy:", accuracy_score(y_test, y_pred_nb))
print(classification_report(y_test, y_pred_nb))

# CELL 9: Gradient Boosting Classification
from sklearn.ensemble import GradientBoostingClassifier

gb_model = GradientBoostingClassifier()
gb_model.fit(X_train_embed, y_train)
y_pred_gb = gb_model.predict(X_test_embed)

print("Gradient Boosting Accuracy:", accuracy_score(y_test, y_pred_gb))
print(classification_report(y_test, y_pred_gb))

# CELL 10: Visualization of Model Performance
import matplotlib.pyplot as plt

models = ['Logistic Regression', 'Random Forest', 'SVM', 'Naive Bayes', 'Gradient Boosting']
accuracies = [
    accuracy_score(y_test, y_pred_lr),
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_svm),
    accuracy_score(y_test, y_pred_nb),
    accuracy_score(y_test, y_pred_gb)
]

plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color='skyblue')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.ylim([0, 1])
plt.xticks(rotation=15)
plt.grid(axis='y')
plt.show()